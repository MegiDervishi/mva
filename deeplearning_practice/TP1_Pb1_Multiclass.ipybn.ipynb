{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-classification problem\n",
    "\n",
    "__Note:__ you might need to do\n",
    "`conda install torchvision \"pillow>7\"`\n",
    "if torchvision is not already installed on your computer, and/or for compatibility issues (the version of torchvision version supporting the last version of Pillow is not released yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "PyTorch provides two powerful data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as prepare your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "### USPS Dataset\n",
    "* Handwritten digits with 10 classes\n",
    "* 16x16 pixels for each image \n",
    "* 6 000 data examples in training set, 1 291 examples in validation set, 2 007 in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6579383"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "if not os.path.isdir('USPS/'):\n",
    "    os.mkdir('USPS/')\n",
    "open('USPS/usps.bz2', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading USPS dataset from torchvision.dataset\n",
    "dataset = torchvision.datasets.USPS(root='USPS/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset USPS\n",
       "    Number of datapoints: 7291\n",
       "    Root location: USPS/\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get info from dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the inputs and targets:\n",
    "inputs = dataset.data\n",
    "targets = dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs = (7291, 16, 16), Targets = 7291\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs = {inputs.shape}, Targets = {len(targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to training and validation sets\n",
    "train_set, val_set = random_split(dataset, [6000, 1291])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'image label: 9')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 88\n",
    "plt.imshow(dataset.data[sample_index], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % dataset.targets[sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "The `torch.nn` namespace provides all the building blocks you need to create your own neural network such as fully connected layers or convolutional layers etc. We define our neural network by subclassing `nn.Module`, and the neural network layers are initialized in **\\__init\\__**. Every `nn.Module` subclass implements the operations on input data in the **forward** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(16*16, 100)\n",
    "        self.l2 = nn.Linear(100, 10)\n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.l1(inputs))\n",
    "        outputs = F.softmax(self.l2(h), dim=1)# Use softmax as the activation function for the last layer\n",
    "        return outputs\n",
    "\n",
    "class Model2(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # ?, 20, 12, 12\n",
    "        self.l1 = nn.Linear(2880, 10) #  # ? 2880 = 20*12*12 , 10\n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.conv1(inputs))\n",
    "        h = h.view(-1, 2880) \n",
    "        outputs = F.softmax(self.l1(h), dim=1)# Use softmax as the activation function for the last layer\n",
    "        return outputs\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # ?, 20, 12, 12\n",
    "        self.conv2 = nn.Conv2d(20,10,5) # ?, 10, 8, 8\n",
    "        self.l1 = nn.Linear(640, 320) # ? 640 , 320\n",
    "        self.l2 = nn.Linear(320, 10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.conv1(inputs))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(-1, 640)\n",
    "        h = F.relu(self.l1(h))\n",
    "        outputs = F.softmax(self.l2(h), dim=1)# Use softmax as the activation function for the last layer\n",
    "        return outputs\n",
    "\n",
    "class Model4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model4, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # ?, 20, 12, 12\n",
    "        self.conv2 = nn.Conv2d(20,10,5) # ?, 10, 8, 8\n",
    "        self.l1 = nn.Linear(640, 320) # ? 640 , 320\n",
    "        self.l2 = nn.Linear(320, 10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.conv1(inputs))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(-1, 640)\n",
    "        h = F.relu(self.l1(h))\n",
    "        outputs = self.l2(h) # to use nn.crosentropyloss no activation function \n",
    "        return outputs\n",
    "    \n",
    "class Model5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model5, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3) # ?, 32, 14, 14\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32,64,3) # ?, 64, 12, 12\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64,128,3 ) # ?, 128, 10, 10\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128,176,3) # ?, 176, 8, 8\n",
    "        self.norm4 = nn.BatchNorm2d(176)\n",
    "        self.l1 = nn.Linear(11264, 10) # ? 640 , 320\n",
    "        self.norm5 = nn.BatchNorm1d(10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.norm1(self.conv1(inputs)))\n",
    "        h = F.relu(self.norm2(self.conv2(h)))\n",
    "        h = F.relu(self.norm3(self.conv3(h)))\n",
    "        h = F.relu(self.norm4(self.conv4(h)))\n",
    "        h = h.view(-1, 11264)\n",
    "        #h = F.relu(self.l1(h))\n",
    "        outputs = self.norm5(self.l1(h)) # to use nn.crosentropyloss no activation function \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Model6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model6, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3) # ?, 32, 14, 14\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32,64,3) # ?, 64, 12, 12\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64,128,3 ) # ?, 128, 10, 10\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128,176,3) # ?, 176, 8, 8\n",
    "        self.norm4 = nn.BatchNorm2d(176)\n",
    "        self.l1 = nn.Linear(11264, 10) # ? 640 , 320\n",
    "        self.norm5 = nn.BatchNorm1d(10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.norm1(self.conv1(inputs)))\n",
    "        h = F.relu(self.norm2(self.conv2(h)))\n",
    "        h = F.relu(self.norm3(self.conv3(h)))\n",
    "        h = F.relu(self.norm4(self.conv4(h)))\n",
    "        h = h.view(-1, 11264)\n",
    "        #h = F.relu(self.l1(h))\n",
    "        outputs = self.norm5(self.l1(h)) # to use nn.crosentropyloss no activation function \n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model6(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 176, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm4): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=11264, out_features=10, bias=True)\n",
       "  (norm5): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: \n",
    "model = Model6()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the hyperparameters for training: \n",
    "num_epochs = 30\n",
    "batch_size = 50\n",
    "\n",
    "# Use mean squared loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for training\n",
    "def train2(num_epochs, batch_size, criterion, optimizer, model, dataset):\n",
    "    train_error = []\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    a = next(iter(train_loader))\n",
    "    print(a[0].shape)\n",
    "    print(a[0].view(batch_size,-1).shape)\n",
    "    input()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_average_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            #y_pre = model(images.view(batch_size, -1)) \n",
    "            #reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape]\n",
    "            y_pre = model(images)\n",
    "            print(y_pre.shape)\n",
    "            input()\n",
    "            # One-hot encoding or labels so as to calculate MSE error:\n",
    "            labels_one_hot = torch.FloatTensor(batch_size, 10)\n",
    "            labels_one_hot.zero_()\n",
    "            labels_one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "            #print(labels_one_hot.shape)\n",
    "            \n",
    "            loss = criterion(y_pre, labels_one_hot)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "        train_error.append(epoch_average_loss)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
    "    return train_error\n",
    "\n",
    "def train(num_epochs, batch_size, criterion, optimizer, model, dataset):\n",
    "    train_error = []\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    a = next(iter(train_loader))\n",
    "    print(a[0].shape)\n",
    "    print(a[0].view(batch_size,-1).shape)\n",
    "    input()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_average_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            #y_pre = model(images.view(batch_size, -1)) \n",
    "            #reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape]\n",
    "            y_pre = model(images)\n",
    "            #print(y_pre.shape)\n",
    "            #input()\n",
    "            # One-hot encoding or labels so as to calculate MSE error:\n",
    "            #labels_one_hot = torch.FloatTensor(batch_size, 10)\n",
    "            #labels_one_hot.zero_()\n",
    "            #labels_one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "            #print(labels_one_hot.shape)\n",
    "            \n",
    "            loss = criterion(y_pre, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "        train_error.append(epoch_average_loss)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
    "    return train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 16, 16])\n",
      "torch.Size([50, 256])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.0095\n",
      "Epoch [2/30], Loss: 0.0067\n",
      "Epoch [3/30], Loss: 0.0047\n",
      "Epoch [4/30], Loss: 0.0039\n",
      "Epoch [5/30], Loss: 0.0045\n",
      "Epoch [6/30], Loss: 0.0042\n",
      "Epoch [7/30], Loss: 0.0031\n",
      "Epoch [8/30], Loss: 0.0039\n",
      "Epoch [9/30], Loss: 0.0039\n",
      "Epoch [10/30], Loss: 0.0028\n",
      "Epoch [11/30], Loss: 0.0029\n",
      "Epoch [12/30], Loss: 0.0028\n",
      "Epoch [13/30], Loss: 0.0025\n",
      "Epoch [14/30], Loss: 0.0021\n",
      "Epoch [15/30], Loss: 0.0016\n",
      "Epoch [16/30], Loss: 0.0023\n",
      "Epoch [17/30], Loss: 0.0029\n",
      "Epoch [18/30], Loss: 0.0033\n",
      "Epoch [19/30], Loss: 0.0019\n",
      "Epoch [20/30], Loss: 0.0024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w_/rqmyx_996rb3rnr0cr9lphjm0000gn/T/ipykernel_39983/1494930760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/w_/rqmyx_996rb3rnr0cr9lphjm0000gn/T/ipykernel_39983/2441327920.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, criterion, optimizer, model, dataset)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m#y_pre = model(images.view(batch_size, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m#print(y_pre.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#input()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w_/rqmyx_996rb3rnr0cr9lphjm0000gn/T/ipykernel_39983/3197056798.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11264\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 16, 16])\n",
      "torch.Size([10, 256])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.8076\n",
      "Epoch [2/20], Loss: 0.5860\n",
      "Epoch [3/20], Loss: 0.4637\n",
      "Epoch [4/20], Loss: 0.3691\n",
      "Epoch [5/20], Loss: 0.2992\n",
      "Epoch [6/20], Loss: 0.2531\n",
      "Epoch [7/20], Loss: 0.2102\n",
      "Epoch [8/20], Loss: 0.1881\n",
      "Epoch [9/20], Loss: 0.1584\n",
      "Epoch [10/20], Loss: 0.1481\n",
      "Epoch [11/20], Loss: 0.1212\n",
      "Epoch [12/20], Loss: 0.1165\n",
      "Epoch [13/20], Loss: 0.0908\n",
      "Epoch [14/20], Loss: 0.0931\n",
      "Epoch [15/20], Loss: 0.0897\n",
      "Epoch [16/20], Loss: 0.0790\n",
      "Epoch [17/20], Loss: 0.0776\n",
      "Epoch [18/20], Loss: 0.0710\n",
      "Epoch [19/20], Loss: 0.0609\n",
      "Epoch [20/20], Loss: 0.0620\n"
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Visualization of convergence')"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training error wrt. the number of epochs: \n",
    "plt.plot(range(1, num_epochs+1), train_error)\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"Train error\")\n",
    "plt.title(\"Visualization of convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy to evaluate the model\n",
    "def accuracy(dataset, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        dataloader = DataLoader(dataset)\n",
    "        model.eval()\n",
    "        for images, labels in dataloader:\n",
    "            #images = images.view(-1, 16*16)\n",
    "            #print(images.shape)\n",
    "            outputs = model(images)\n",
    "            #print(outputs.shape)\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model : {:.2f} %'.format(100*correct.item()/ len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model : 98.84 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(val_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Prediction label: 9')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_index = 66\n",
    "\n",
    "(image, label) = val_set[val_index]\n",
    "print(image.shape)\n",
    "print(image.view(-1,256).shape)\n",
    "print(image.view(16,16).shape)\n",
    "output = model(image.view(1,1,16,16))\n",
    "_, prediction = torch.max(output.data, 1)\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"Prediction label: %d\" % prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of the architecture of the model\n",
    "Define your own class `Model` to improve the predictions:\n",
    "\n",
    "* The convolutional layer can be a good choice to deal with images. Replace nn.Linear with [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d).\n",
    "* Try to add more layers (1, 2, 3, more ?)\n",
    "* Change the number of neurons in hidden layers (5, 10, 20, more ?)\n",
    "* Try different activation functions such as [sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Impact of the optimizer\n",
    "Retrain the model by using different parameters of the optimizer; you can change its parameters in the cell initializing it, after the definition of your model.\n",
    "\n",
    "* Use different batch sizes, from 10 to 1 000 for instance\n",
    "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process. Do all network architectures react the same way to different learning rates?\n",
    "* Change the duration of the training by increasing the number of epochs\n",
    "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of the loss function\n",
    "The MSE error is rarely used in this case. The cross entropy loss can be a better choice for multi-classification problems. In pytorch, the cross entropy loss is defined by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Replace the MSE loss by this one to observe its impact.\n",
    "\n",
    "**Note:** In order to use nn.CrossEntropyLoss correctly, don't add an activation function to the last layer of your network. And one-hot encoding is no longer needed to calculate the loss, delete the encoding procedures in function `train`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction on test set\n",
    "\n",
    "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831726"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('USPS/usps.t.bz2', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST test set from torchvision.dataset\n",
    "test_set = torchvision.datasets.USPS(root='USPS/',\n",
    "                                         train=False,\n",
    "                                         transform=transforms.ToTensor(),\n",
    "                                         download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model : 96.71 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
