{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-classification problem\n",
    "\n",
    "__Note:__ you might need to do\n",
    "`conda install torchvision \"pillow>7\"`\n",
    "if torchvision is not already installed on your computer, and/or for compatibility issues (the version of torchvision version supporting the last version of Pillow is not released yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "PyTorch provides two powerful data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as prepare your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "### USPS Dataset\n",
    "* Handwritten digits with 10 classes\n",
    "* 16x16 pixels for each image \n",
    "* 6 000 data examples in training set, 1 291 examples in validation set, 2 007 in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6579383"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "if not os.path.isdir('USPS/'):\n",
    "    os.mkdir('USPS/')\n",
    "open('USPS/usps.bz2', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading USPS dataset from torchvision.dataset\n",
    "dataset = torchvision.datasets.USPS(root='USPS/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset USPS\n",
       "    Number of datapoints: 7291\n",
       "    Root location: USPS/\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get info from dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the inputs and targets:\n",
    "inputs = dataset.data\n",
    "targets = dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs = (7291, 16, 16), Targets = 7291\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs = {inputs.shape}, Targets = {len(targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to training and validation sets\n",
    "train_set, val_set = random_split(dataset, [6000, 1291])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'image label: 9')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 88\n",
    "plt.imshow(dataset.data[sample_index], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % dataset.targets[sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "The `torch.nn` namespace provides all the building blocks you need to create your own neural network such as fully connected layers or convolutional layers etc. We define our neural network by subclassing `nn.Module`, and the neural network layers are initialized in **\\__init\\__**. Every `nn.Module` subclass implements the operations on input data in the **forward** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(16*16, 100)\n",
    "        self.l2 = nn.Linear(100, 10)\n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.l1(inputs))\n",
    "        outputs = F.softmax(self.l2(h), dim=1)# Use softmax as the activation function for the last layer\n",
    "        return outputs\n",
    "\n",
    "class Model2(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # ?, 20, 12, 12\n",
    "        self.l1 = nn.Linear(2880, 10) #  # ? 2880 = 20*12*12 , 10\n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.conv1(inputs))\n",
    "        h = h.view(-1, 2880) \n",
    "        outputs = F.softmax(self.l1(h), dim=1)# Use softmax as the activation function for the last layer\n",
    "        return outputs\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # ?, 20, 12, 12\n",
    "        self.conv2 = nn.Conv2d(20,10,5) # ?, 10, 8, 8\n",
    "        self.l1 = nn.Linear(640, 320) # ? 640 , 320\n",
    "        self.l2 = nn.Linear(320, 10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.conv1(inputs))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(-1, 640)\n",
    "        h = F.relu(self.l1(h))\n",
    "        outputs = F.softmax(self.l2(h), dim=1)# Use softmax as the activation function for the last layer\n",
    "        return outputs\n",
    "\n",
    "class Model4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model4, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # ?, 20, 12, 12\n",
    "        self.conv2 = nn.Conv2d(20,10,5) # ?, 10, 8, 8\n",
    "        self.l1 = nn.Linear(640, 320) # ? 640 , 320\n",
    "        self.l2 = nn.Linear(320, 10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.conv1(inputs))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = h.view(-1, 640)\n",
    "        h = F.relu(self.l1(h))\n",
    "        outputs = self.l2(h) # to use nn.crosentropyloss no activation function \n",
    "        return outputs\n",
    "    \n",
    "class Model5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model5, self).__init__() # input ?, 1, 16,16\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3) # ?, 32, 14, 14\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32,64,3) # ?, 64, 12, 12\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64,128,3 ) # ?, 128, 10, 10\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128,176,3) # ?, 176, 8, 8\n",
    "        self.norm4 = nn.BatchNorm2d(176)\n",
    "        self.l1 = nn.Linear(11264, 10) # ? 640 , 320\n",
    "        self.norm5 = nn.BatchNorm1d(10) # > 320, 10\n",
    "        \n",
    "        # Input size is 16*16, output size should be the same with the number of classes\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = F.relu(self.norm1(self.conv1(inputs)))\n",
    "        h = F.relu(self.norm2(self.conv2(h)))\n",
    "        h = F.relu(self.norm3(self.conv3(h)))\n",
    "        h = F.relu(self.norm4(self.conv4(h)))\n",
    "        h = h.view(-1, 11264)\n",
    "        #h = F.relu(self.l1(h))\n",
    "        outputs = self.norm5(self.l1(h)) # to use nn.crosentropyloss no activation function \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model5(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 176, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm4): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=11264, out_features=10, bias=True)\n",
       "  (norm5): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model: \n",
    "model = Model5()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the hyperparameters for training: \n",
    "num_epochs = 1\n",
    "batch_size = 10\n",
    "\n",
    "# Use mean squared loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for training\n",
    "def train2(num_epochs, batch_size, criterion, optimizer, model, dataset):\n",
    "    train_error = []\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    a = next(iter(train_loader))\n",
    "    print(a[0].shape)\n",
    "    print(a[0].view(batch_size,-1).shape)\n",
    "    input()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_average_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            #y_pre = model(images.view(batch_size, -1)) \n",
    "            #reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape]\n",
    "            y_pre = model(images)\n",
    "            print(y_pre.shape)\n",
    "            input()\n",
    "            # One-hot encoding or labels so as to calculate MSE error:\n",
    "            labels_one_hot = torch.FloatTensor(batch_size, 10)\n",
    "            labels_one_hot.zero_()\n",
    "            labels_one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "            #print(labels_one_hot.shape)\n",
    "            \n",
    "            loss = criterion(y_pre, labels_one_hot)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "        train_error.append(epoch_average_loss)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
    "    return train_error\n",
    "\n",
    "def train(num_epochs, batch_size, criterion, optimizer, model, dataset):\n",
    "    train_error = []\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    a = next(iter(train_loader))\n",
    "    print(a[0].shape)\n",
    "    print(a[0].view(batch_size,-1).shape)\n",
    "    input()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_average_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            #y_pre = model(images.view(batch_size, -1)) \n",
    "            #reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape]\n",
    "            y_pre = model(images)\n",
    "            #print(y_pre.shape)\n",
    "            #input()\n",
    "            # One-hot encoding or labels so as to calculate MSE error:\n",
    "            #labels_one_hot = torch.FloatTensor(batch_size, 10)\n",
    "            #labels_one_hot.zero_()\n",
    "            #labels_one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "            #print(labels_one_hot.shape)\n",
    "            \n",
    "            loss = criterion(y_pre, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "        train_error.append(epoch_average_loss)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
    "    return train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 16, 16])\n",
      "torch.Size([10, 256])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.7986\n"
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 16, 16])\n",
      "torch.Size([10, 256])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.8076\n",
      "Epoch [2/20], Loss: 0.5860\n",
      "Epoch [3/20], Loss: 0.4637\n",
      "Epoch [4/20], Loss: 0.3691\n",
      "Epoch [5/20], Loss: 0.2992\n",
      "Epoch [6/20], Loss: 0.2531\n",
      "Epoch [7/20], Loss: 0.2102\n",
      "Epoch [8/20], Loss: 0.1881\n",
      "Epoch [9/20], Loss: 0.1584\n",
      "Epoch [10/20], Loss: 0.1481\n",
      "Epoch [11/20], Loss: 0.1212\n",
      "Epoch [12/20], Loss: 0.1165\n",
      "Epoch [13/20], Loss: 0.0908\n",
      "Epoch [14/20], Loss: 0.0931\n",
      "Epoch [15/20], Loss: 0.0897\n",
      "Epoch [16/20], Loss: 0.0790\n",
      "Epoch [17/20], Loss: 0.0776\n",
      "Epoch [18/20], Loss: 0.0710\n",
      "Epoch [19/20], Loss: 0.0609\n",
      "Epoch [20/20], Loss: 0.0620\n"
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Visualization of convergence')"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEXCAYAAAC3c9OwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgdUlEQVR4nO3de5gdVZnv8e+PhBCQcDOtI+ncxGAIDoJuAjoyIBiN0SHeRhJFiIcRZpQcRDgaZxjN5MgwOioHjxEmKCIoxMgDnlbBgBhEMEh2TLgkMdBESDpcbMAAkYEQ8p4/arUpdlc3O91dvbuT3+d56umqtVbVftfuZL9dq2qvUkRgZmZWa7dGB2BmZgOTE4SZmRVygjAzs0JOEGZmVsgJwszMCjlBmJlZIScI2yGSVkk6ruTXCEmvS+uXSPrXEl7jBkmn9vVx63jdL0l6XNKj/f3aZjtK/h6EdZD0c+DOiPhCTfl04L+A5ojY2g9xBDAhIlr76HhzgddFxMl9cbxexDEGWAuMjYg/NjIWs3r4DMLyvgecLEk15R8DftAfyWEnNwZ4YrAmB0lDGx2D9S8nCMv7MfBK4JiOAkn7A+8FrkjbD0p6R1qfLKkq6WlJj0n6eio/TlJb/sAF+y2VtEnSI5K+KWlYUUCSLpf0pbT+E0mbc8s2SbNS3UWSNqRYlks6JpVPBf4ZOCntc1cqv0XSP6T13SSdJ+khSX+UdIWkfVPduDTkdaqk9Wl46F+6egMl7Zv2b0/HOy8d/x3ATcCBKY7Lu9h/uqSVqR8PpPiRdKCkFklPSmqV9IncPnMlLUqv+0waBqykus9JuqbmNS6S9I1cvN9Jv4eNaQhsSKqbJel2SRdKegKYK+mV6ffwtKRlqf1tuWNPlHRTinOtpA/X/C7nS/pZivO3kg7K1R+a2/cxSf+c+/3MSe/HE6mvB3T1O7A+FBFevPxlAS4Fvp3bPgNYmdt+EHhHWl8KfCyt7w0cndaPA9pqjpvf783A0cBQYBywBvh0rm2QDQkBXA58qSDOdwMPA6PT9slkyW0ocA7wKDA81c0Fvl+z/y3AP6T1/wG0Aq9N/bgWuDLVjUvxXArsCbwReB44pIv37wrg/wEj0r73Aad19b7U7DsZeAqYQvbH2yhgYqq7FfgWMBw4HGgHjs/17zlgGjAEuAC4I9WNBZ4FRqTtIcAjud/VdWTDh68AXgXcCZyR6mYBW4HZ6X3dE1iYlr2AScAG4LbU/hVp++Op/RHA48Ck3O/yidTPocAPgIWpbkSK65zUxxHAUanuLOAOoBnYI8V7daP/r+wKS8MD8DKwFuBtwKbch+vtwNm5+gfZ/kF/K/BvwMiaY3T6IMzvV/Canwauy213myCAg4E/Am/rph9/At6Y1ufSfYK4Gfhkru71wAtsT2BBdv2lo/5OYEbBaw4BtnR8IKayM4Bbunpfavb/L+DCgvLRwIukD/lUdgFwea5/v8jVTQL+O7d9G3BKWp8CPJDWX02W7PbMtZ0JLEnrs4D1Nf17AXh9ruxLbE8QJwG/LujTF3O/y/wfH9OA3+ded0UX78sa4ITc9ms6fj+N/v+ysy8eYrKXiIjbyP7qe186/Z8MXNVF89PIPqx/n4Yb3lvPa0g6WNJPJT0q6Wng34GRde67L9lf6OelWDvKz5W0RtJTkjYB+9Z7TOBA4KHc9kNkyeHVubL8XUfPkp1p1BoJ7F5wrFF1xjEaeKCL+J6MiGe6OW5tfMO1/ZrBVWQfwAAfYfvvc2yK95E03LeJ7AP9VbljbcitN5G9Lxu6qB8LHNVxrHS8jwJ/1U2cHe9jV33vOO51uWOuIUuYr+6ivfURJwgrcgVwCtmwzeKIeKyoUUTcHxEzyT5QvgxcI+kVwJ/JhiAASGPaTbldLwZ+T3an0j5k1whqL4x3Imk3sg+3JRGxIFd+DPBZ4MPA/hGxH9lQTccxX+5WvYfJPoQ6jCEbWinsdzceJ/vLtvZYG+vcfwNwUEH5w8ABkkb08Lg/Ao6T1Ay8n+0JYgPZGcTIiNgvLftExKG5ffPvXTvZ+9KcKxtdE/+vcsfaLyL2joh/qiPGDWRDfF3VvbvmuMMjot7+Ww85QViRK4B3AJ8gu7OpkKSTJTVFxDayYSmAbWTj7sMlvUfS7sB5ZGPHHUYATwObJU0E6vkAATifbJz7rJryEWQfXO3AUElfAPbJ1T8GjEsJpsjVwNmSxkvam+yM5oexg3dtRcSLwCLgfEkjJI0FPgN8v85DfAf4uKQT0oXZUZImRsQG4DfABZKGSzqM7OytruNGRDvZkNp3gT9ExJpU/ghwI/A1Sfuk1zxI0rHd9O9asovVe6Xf3Sm5Jj8FDpb0MUm7p+VISYfUEeZPgddI+rSkPdL7d1Squ4TsPR0LIKlJ2a3XVjInCOskIh4k+0B6BdDSTdOpwCpJm4GLyMbl/zsingI+CXyb7K/cPwP5u5rOJRvqeIbs4u8P6wxtJtnF7T9p+51MHwUWAz8nS0wPkV2wzQ99/Cj9fELS7wqOexlwJdk1lT+k/WfXGVOt2WT9XUc29n9VOv7Liog7yS7wXkh2BvQrtp+NzCS7HvIw2YXlL0bEL3YgrqvIkn7tcOEpwDBgNdl1m2vIxvi7cibZ8N2jZO/Z1WRnIaQhsHcCM1Kcj5KdWe5ReKSctO8U4O/SfvcDb0/VF5H9O7xR0jNkF6yPKjqO9S1/Uc7MekzSl4G/ioh+/1a6lc9nEGZWt/Q9h8OUmUw21HVdo+OycvibkWa2I0aQDSsdSHZt52tkd5XZTshDTGZmVshDTGZmVminGWIaOXJkjBs3rtFhmJkNKsuXL388IpqK6naaBDFu3Diq1WqjwzAzG1QkPdRVnYeYzMysUKkJQtLUNOVvq6Q5BfVjJC2RtELS3ZKmFdRvlnRumXGamVlnpSWINP/OfLJpmScBMyVNqml2HrAoIo4g+/blt2rqvw7cUFaMZmbWtTLPICYDrRGxLiK2kM0hXzt/SrB9zpx9yb6eD4Ck95FNe7CqxBjNzKwLZSaIUbx0Ppw2Ok97PJfsEZdtwPWk+W/ShGmfI3vWgJmZNUCjL1LPJHvoSTPZw0OuTDNuziV7cMrm7naWdLqyR15W29vby4/WzGwXUuZtrht56VzxzXSev/40shlBiYilkoaTPXTlKOBDkr4C7Adsk/RcRHwzv3N6JsACgEql4q+Em5n1oTITxDJggqTxZIlhBtkUz3nrgROAy9Oc8cOB9og4pqOBpLnA5trkYGZm5SptiCk9bOVMsrn615DdrbRK0jxJJ6Zm5wCfkHQX2QRgs8KTQ5mZDQg7zWR9lUol/E1qM7MdI2l5RFSK6hp9kdrMzAYoJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWyAnCzMwKlZogJE2VtFZSq6Q5BfVjJC2RtELS3ZKmpfLJklam5S5J7y8zTjMz66y0Z1JLGgLMB6YAbcAySS0RsTrX7DyyR5FeLGkScD0wDrgXqETEVkmvAe6S9JP0GFMzM+sHZZ5BTAZaI2JdRGwBFgLTa9oEsE9a3xd4GCAins0lg+GpnZmZ9aMyE8QoYENuuy2V5c0FTpbURnb2MLujQtJRklYB9wD/WHT2IOl0SVVJ1fb29r6O38xsl9boi9QzgcsjohmYBlwpaTeAiPhtRBwKHAl8XtLw2p0jYkFEVCKi0tTU1K+Bm5nt7MpMEBuB0bnt5lSWdxqwCCAilpINJ43MN4iINcBm4A2lRWpmZp2UmSCWARMkjZc0DJgBtNS0WQ+cACDpELIE0Z72GZrKxwITgQdLjNXMzGqUdhdTugPpTGAxMAS4LCJWSZoHVCOiBTgHuFTS2WQXomdFREh6GzBH0gvANuCTEfF4WbGamVlnitg5bhCqVCpRrVYbHYaZ2aAiaXlEVIrqGn2R2szMBignCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAqVmiAkTZW0VlKrpDkF9WMkLZG0QtLdkqal8imSlku6J/08vsw4zcyss9IeOSppCDAfmAK0AcsktUTE6lyz84BFEXGxpEnA9cA44HHg7yLiYUlvIHts6aiyYjUzs87KPIOYDLRGxLqI2AIsBKbXtAlgn7S+L/AwQESsiIiHU/kqYE9Je5QYq5mZ1SgzQYwCNuS22+h8FjAXOFlSG9nZw+yC43wQ+F1EPF9bIel0SVVJ1fb29r6J2szMgMZfpJ4JXB4RzcA04EpJf4lJ0qHAl4EzinaOiAURUYmISlNTU78EbGa2qygzQWwERue2m1NZ3mnAIoCIWAoMB0YCSGoGrgNOiYgHSozTzMwKlJkglgETJI2XNAyYAbTUtFkPnAAg6RCyBNEuaT/gZ8CciLi9xBjNzKwLpSWIiNgKnEl2B9IasruVVkmaJ+nE1Owc4BOS7gKuBmZFRKT9Xgd8QdLKtLyqrFjNzKwzZZ/Hg1+lUolqtdroMMzMBhVJyyOiUlTX6IvUZmY2QDlBmJlZIScIMzMr5ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGZmVsgJwszMCjlBmJlZIScIMzMr5ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGZmVqjUBCFpqqS1klolzSmoHyNpiaQVku6WNC2VvzKVb5b0zTJjNDOzYqUlCElDgPnAu4FJwExJk2qanUf2KNIjyJ5Z/a1U/hzwr8C5ZcVnZmbdK/MMYjLQGhHrImILsBCYXtMmgH3S+r7AwwAR8eeIuI0sUZiZWQOUmSBGARty222pLG8ucLKkNuB6YPaOvICk0yVVJVXb29t7E6uZmdVo9EXqmcDlEdEMTAOulFR3TBGxICIqEVFpamoqLUgzs11RmQliIzA6t92cyvJOAxYBRMRSYDgwssSYzMysTmUmiGXABEnjJQ0juwjdUtNmPXACgKRDyBKEx4rMzAaAod1VpuGeoyPiNzt64IjYKulMYDEwBLgsIlZJmgdUI6IFOAe4VNLZZBesZ0VEpNd+kOwC9jBJ7wPeGRGrdzQOMzPrGaXP464bSCvSbagDWqVSiWq12ugwzMwGFUnLI6JSVFfPENPNkj4oSX0cl5mZDWD1JIgzgB8BWyQ9LekZSU+XHJeZmTVYt9cgACJiRH8EYmZmA8vLJggASScCf5s2b4mIn5YXkpmZDQQvO8Qk6T+As4DVaTlL0gVlB2ZmZo1VzxnENODwiNgGIOl7wArg82UGZmZmjVXvF+X2y63vW0IcZmY2wNRzBvHvwApJSwCRXYvo9GwHMzPbudTzTeptwNHAkan4cxHxaNmBmZlZY3WbICJim6TPRsQiOs+jZGZmO7F6rkH8QtK5kkZLOqBjKT0yMzNrqHquQZyUfn4qVxbAa/s+HDMzGyjquQYxJyJ+2E/xmJnZANHtEFP67sP/6qdYzMxsAPE1CDMzK1RPgjiJ7PrDrcDytNT14AVJUyWtldQqqdN3JySNkbRE0gpJd0ualqv7fNpvraR31dcdMzPrK/XM5jq+JweWNASYD0wB2oBlklpqngp3HrAoIi6WNAm4HhiX1mcAhwIHkp3FHBwRL/YkFjMz23H1TNa3l6TzJC1I2xMkvbeOY08GWiNiXURsARYC02vaBNljRSGbwuPhtD4dWBgRz0fEH4DWdDwzM+sn9QwxfRfYArw1bW8EvlTHfqOADbnttlSWNxc4WVIb2dnD7B3Y18zMSlRPgjgoIr4CvAAQEc+SzcnUF2YCl0dEM9mssVemW2vrIul0SVVJ1fb29j4KyczMoL4EsUXSnmTDQUg6CHi+jv02AqNz282pLO80YBFARCwFhgMj69yXiFgQEZWIqDQ1NdURkpmZ1aueBPFF4OfAaEk/AG4GPlvHfsuACZLGSxpGdtG5dj6n9cAJAJIOIUsQ7andDEl7SBoPTADurOM1zcysj9RzF9NNkn5HNqOrgLMi4vE69tsq6UxgMTAEuCwiVkmaB1QjogU4B7hU0tlkZyizIiKAVZIWkT3BbivwKd/BZGbWv5R9Hg9+lUolqtW6vp5hZmaJpOURUSmqq/uCsJmZ7VqcIMzMrFA90313fCv61fn2EbG+rKDMzKzxXjZBSJpNdifTY2SPH4XsgvJhJcZlZmYNVs8ZxFnA6yPiibKDMTOzgaOeaxAbgKfKDsTMzAaWes4g1gG3SPoZuW9QR8TXS4vKzMwarp4EsT4tw9JiZma7gHq+Sf1v/RGImZkNLF0mCEn/JyI+LeknpIn68iLixFIjMzOzhuruDOLK9POr/RGImZkNLF0miIhYnn7+qv/CMTOzgaKeL8pNAC4AJpFNxw1ARLy2xLjMzKzB6n3k6MVk026/HbgC+H6ZQZmZWePVkyD2jIibyaYGfygi5gLvKTcsMzNrtHq+B/F8ek70/ekBQBuBvcsNy8zMGq2eM4izgL2A/wm8GTgZOLWeg0uaKmmtpFZJcwrqL5S0Mi33SdqUq/uypHvTclJdvTEzsz7T7RlEmub7pIg4F9gMfLzeA6d95wNTgDZgmaSWiFjd0SYizs61nw0ckdbfA7wJOBzYg2yqjxsi4ul6X9/MzHqnyzMISUPTc6Df1sNjTwZaI2JdRGwBFgLTu2k/E7g6rU8Cbo2IrRHxZ+BuYGoP4zAzsx7obojpzvRzhaQWSR+T9IGOpY5jjyKbCbZDWyrrRNJYYDzwy1R0FzBV0l6SRpLdPTW6YL/TJVUlVdvb2+sIyczM6lXPRerhwBPA8WRTbij9vLYP45gBXJPOWIiIGyUdCfwGaAeWAi/W7hQRC4AFAJVKpdN0IGZm1nPdJYhXSfoMcC/bE0OHej6MN/LSv/qbU1mRGcCn8gURcT5wPoCkq4D76nhNMzPrI90liCFkt7OqoK6eBLEMmCBpPFlimAF8pLaRpInA/mRnCR1lQ4D9IuIJSYeRPd70xjpe08zM+kh3CeKRiJjX0wNHxNb0vYnFZMnmsohYJWkeUI2IltR0BrAwIvJJZ3fg15IAngZOjoitPY3FzMx2XHcJoujMYYdExPXA9TVlX6jZnluw33NkdzKZmVmDdHcX0wn9FoWZmQ04XSaIiHiyPwMxM7OBpZ6pNszMbBfkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWqNQEIWmqpLWSWiXNKai/UNLKtNwnaVOu7iuSVklaI+kbSo+XMzOz/tHdE+V6JT1Xej4wBWgDlklqiYjVHW0i4uxc+9nAEWn9rcDfkD2LGuA24FjglrLiNTOzlyrzDGIy0BoR6yJiC7AQmN5N+5nA1Wk9gOHAMGAPsmdUP1ZirGZmVqPMBDEK2JDbbktlnUgaC4wHfgkQEUuBJcAjaVkcEWsK9jtdUlVStb29vY/DNzPbtQ2Ui9QzgGsi4kUASa8DDgGayZLK8ZKOqd0pIhZERCUiKk1NTf0asJnZzq7MBLERGJ3bbk5lRWawfXgJ4P3AHRGxOSI2AzcAbyklSjMzK1RmglgGTJA0XtIwsiTQUttI0kRgf2Bprng9cKykoZJ2J7tA3WmIyczMylNagoiIrcCZwGKyD/dFEbFK0jxJJ+aazgAWRkTkyq4BHgDuAe4C7oqIn5QVq5mZdaaXfi4PXpVKJarVaqPDMDMbVCQtj4hKUd1AuUhtZmYDjBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhZwgzMyskBOEmZkVcoIwM7NCThBmZlbICcLMzAo5QZiZWSEnCDMzK+QEYWZmhUpNEJKmSlorqVXSnIL6CyWtTMt9kjal8rfnyldKek7S+8qM1czMXmpoWQeWNASYD0wB2oBlkloiYnVHm4g4O9d+NnBEKl8CHJ7KDwBagRvLitXMzDor8wxiMtAaEesiYguwEJjeTfuZwNUF5R8CboiIZ0uI0czMulBmghgFbMhtt6WyTiSNBcYDvyyonkFx4kDS6ZKqkqrt7e29DNfMzPIGykXqGcA1EfFivlDSa4C/BhYX7RQRCyKiEhGVpqamfgjTzGzXUWaC2AiMzm03p7IiXZ0lfBi4LiJe6OPYzMzsZZSZIJYBEySNlzSMLAm01DaSNBHYH1hacIyurkuYmVnJSksQEbEVOJNseGgNsCgiVkmaJ+nEXNMZwMKIiPz+ksaRnYH8qqwYzcysa6r5XB60KpVKVKvVRodhZjaoSFoeEZWiuoFykdrMzAYYJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvkBGFmZoWcIMzMrJAThJmZFXKCMDOzQk4QZmZWqNQEIWmqpLWSWiXNKai/UNLKtNwnaVOuboykGyWtkbQ6PUDIzMz6ydCyDixpCDAfmAK0AcsktUTE6o42EXF2rv1s4IjcIa4Azo+ImyTtDWwrK1YzM+uszDOIyUBrRKyLiC3AQmB6N+3/8vxpSZOAoRFxE0BEbI6IZ0uM1czMapSZIEYBG3LbbamsE0ljgfHAL1PRwcAmSddKWiHpP9MZiZmZ9ZOBcpF6BnBNRLyYtocCxwDnAkcCrwVm1e4k6XRJVUnV9vb2/orVzGyXUGaC2AiMzm03p7IiM0jDS0kbsDINT20Ffgy8qXaniFgQEZWIqDQ1NfVN1GZmBpSbIJYBEySNlzSMLAm01DaSNBHYH1has+9+kjo+9Y8HVtfua2Zm5SktQaS//M8EFgNrgEURsUrSPEkn5prOABZGROT2fZFseOlmSfcAAi4tK1YzM+tMuc/lQa1SqUS1Wm10GGZmg4qk5RFRKaobKBepzcxsgHGCMDOzQk4QZmZWyAnCzMwKOUGYmVkhJwgzMyvkBGFmZoWcIMzMrNBO80U5Se3AQ42OowdGAo83Ooh+5j7vGtznwWFsRBROZrfTJIjBSlK1q28x7qzc512D+zz4eYjJzMwKOUGYmVkhJ4jGW9DoABrAfd41uM+DnK9BmJlZIZ9BmJlZIScIMzMr5ARRIklTJa2V1CppTkH9WEk3S7pb0i2SmnN1YyTdKGmNpNWSxvVr8D3Uyz5/RdKq1OdvSFL/Rr/jJF0m6Y+S7u2iXqkvranPb8rVnSrp/rSc2n9R905P+yzpcElL0+/4bkkn9W/kPdeb33Oq30dSm6Rv9k/EfSQivJSwAEOAB4DXAsOAu4BJNW1+BJya1o8HrszV3QJMSet7A3s1uk9l9hl4K3B7OsYQsmeUH9foPtXR578F3gTc20X9NOAGssfmHg38NpUfAKxLP/dP6/s3uj8l9/lgYEJaPxB4BNiv0f0ps8+5+ouAq4BvNrovO7L4DKI8k4HWiFgXEVuAhcD0mjaTgF+m9SUd9ZImAUMj4iaAiNgcEc/2T9i90uM+AwEMJ0ssewC7A4+VHnEvRcStwJPdNJkOXBGZO4D9JL0GeBdwU0Q8GRF/Am4CppYfce/1tM8RcV9E3J+O8TDwR6DwG7wDTS9+z0h6M/Bq4MbyI+1bThDlGQVsyG23pbK8u4APpPX3AyMkvZLsL61Nkq6VtELSf0oaUnrEvdfjPkfEUrKE8UhaFkfEmpLj7Q9dvSf1vFeD1cv2TdJksj8GHujHuMpU2GdJuwFfA85tSFS95ATRWOcCx0paARwLbAReBIYCx6T6I8mGbGY1KMa+VthnSa8DDgGayf6zHS/pmMaFaWVJf1lfCXw8IrY1Op6SfRK4PiLaGh1ITwxtdAA7sY3A6Nx2cyr7i3Sa/QEASXsDH4yITZLagJURsS7V/ZhsXPM7/RB3b/Smz58A7oiIzanuBuAtwK/7I/ASdfWebASOqym/pd+iKleX/w4k7QP8DPiXNBSzs+iqz28BjpH0SbJricMkbY6ITjdwDEQ+gyjPMmCCpPGShgEzgJZ8A0kj0ykowOeBy3L77iepY3z2eGB1P8TcW73p83qyM4uhknYnO7vYGYaYWoBT0l0uRwNPRcQjwGLgnZL2l7Q/8M5UtjMo7HP6N3Ed2Vj9NY0Nsc8V9jkiPhoRYyJiHNnZ8xWDJTmAzyBKExFbJZ1J9p9+CHBZRKySNA+oRkQL2V+QF0gK4FbgU2nfFyWdC9ycbvVcDlzaiH7siN70GbiGLBHeQ3bB+ucR8ZP+7sOOknQ1WZ9GpjO/L5JdYCciLgGuJ7vDpRV4Fvh4qntS0v8mS6oA8yKiu4ugA0ZP+wx8mOxuoFdKmpXKZkXEyv6Kvad60edBzVNtmJlZIQ8xmZlZIScIMzMr5ARhZmaFnCDMzKyQE4SZmRVygjAzs0JOEGaDkKQHJY1sdBy2c3OCMDOzQk4QtsuQNC49jOjS9NCaGyXtqezBRZXUZqSkB9P6LEk/lnRT+ov9TEmfSTPs3iHpgG5e6yBJP5e0XNKvJU1M5ZdLukRSVdJ9kt6byodL+q6ke9Lx357Kh0j6qqR704NoZudeZrak36V9Oo5/rKSVaVkhaUQ576btCpwgbFczAZgfEYcCm4APvkz7N5BNLngkcD7wbEQcQfZAo1O62W8BMDsi3kw2B8+3cnXjyJ6d8R7gEknDyaYciYj4a2Am8L1Ufnpqf3hEHAb8IHecxyPiTcDFbJ9O+lzgUxFxONmMwP/9Mv0z65LnYrJdzR9yc/8sJ/vw7c6SiHgGeEbSU0DH/FD3AIcV7ZBmqX0r8CNtf2rqHrkmi9I01/dLWgdMBN4G/F+AiPi9pIfIngvyDuCSiNia6vLzNV2b60fHMzZuB74u6QfAtYN1mmkbGJwgbFfzfG79RWBPYCvbz6aHd9N+W257G13//9kN2JT+ii9SOwFaTydE64il4xkiRMR/SPoZ2cRxt0t6V0T8vofHt12ch5jM4EHgzWn9Q709WEQ8DfxB0t/DXx5o/8Zck7+XtJukg8geBrWW7LkXH03tDwbGpPKbgDMkDU11XV73SPUHRcQ9EfFlspliJ/a2P7brcoIwg68C/6TsKXd9devoR4HTJN0FrOKlz+ZeD9xJ9pD7f4yI58iuUewm6R7gh2TTYD8PfDu1vzsd6yMv87qf7rigDbyQXsOsRzzdt1k/knQ58NOd8IE5thPyGYSZmRXyGYRZL0iaD/xNTfFFEfHdRsRj1pecIMzMrJCHmMzMrJAThJmZFXKCMDOzQk4QZmZW6P8DAtpWGku2D6sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training error wrt. the number of epochs: \n",
    "plt.plot(range(1, num_epochs+1), train_error)\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"Train error\")\n",
    "plt.title(\"Visualization of convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy to evaluate the model\n",
    "def accuracy(dataset, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        dataloader = DataLoader(dataset)\n",
    "        model.eval()\n",
    "        for images, labels in dataloader:\n",
    "            #images = images.view(-1, 16*16)\n",
    "            #print(images.shape)\n",
    "            outputs = model(images)\n",
    "            #print(outputs.shape)\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model : {:.2f} %'.format(100*correct.item()/ len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model : 96.59 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(val_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Prediction label: 9')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_index = 66\n",
    "\n",
    "(image, label) = val_set[val_index]\n",
    "print(image.shape)\n",
    "print(image.view(-1,256).shape)\n",
    "print(image.view(16,16).shape)\n",
    "output = model(image.view(1,1,16,16))\n",
    "_, prediction = torch.max(output.data, 1)\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"Prediction label: %d\" % prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of the architecture of the model\n",
    "Define your own class `Model` to improve the predictions:\n",
    "\n",
    "* The convolutional layer can be a good choice to deal with images. Replace nn.Linear with [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d).\n",
    "* Try to add more layers (1, 2, 3, more ?)\n",
    "* Change the number of neurons in hidden layers (5, 10, 20, more ?)\n",
    "* Try different activation functions such as [sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Impact of the optimizer\n",
    "Retrain the model by using different parameters of the optimizer; you can change its parameters in the cell initializing it, after the definition of your model.\n",
    "\n",
    "* Use different batch sizes, from 10 to 1 000 for instance\n",
    "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process. Do all network architectures react the same way to different learning rates?\n",
    "* Change the duration of the training by increasing the number of epochs\n",
    "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of the loss function\n",
    "The MSE error is rarely used in this case. The cross entropy loss can be a better choice for multi-classification problems. In pytorch, the cross entropy loss is defined by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Replace the MSE loss by this one to observe its impact.\n",
    "\n",
    "**Note:** In order to use nn.CrossEntropyLoss correctly, don't add an activation function to the last layer of your network. And one-hot encoding is no longer needed to calculate the loss, delete the encoding procedures in function `train`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction on test set\n",
    "\n",
    "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1831726"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('USPS/usps.t.bz2', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST test set from torchvision.dataset\n",
    "test_set = torchvision.datasets.USPS(root='USPS/',\n",
    "                                         train=False,\n",
    "                                         transform=transforms.ToTensor(),\n",
    "                                         download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model : 94.17 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
